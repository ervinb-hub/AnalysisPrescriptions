{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "The main objective for collecting the data is to analyse and predict the usage of a certain medical drug, related to a given geographical area. Other points I am interesetd to find are: popularity of a medical drug and seasonal consumption. To summarise the ideas the questions I am trying to answer are: \n",
    "\n",
    "1. Which is the medication prescribed in a certain area? Once the answer is available by analysing the data, I will be trying to build a ststistical model to predict its future consumption.\n",
    "\n",
    "\n",
    "2. What is the preferred chemical prescribed by GP practices for a given category? For example, the research aims to explore the most prescribed drug for category. Examples of such categories include: \"Hypertension and Heart Failure\", \"Dyspepsia and gastro-oesophageal reflux disease\", etc -- to name a few. A full list is available in the BNF (British National Formulary) document.\n",
    "\n",
    "\n",
    "3. Which medication is more used in a given season? This is an attempt to explore the trend for a medication regarding a specific season. The result will report its category as well.\n",
    "\n",
    "In order to simplify the number of joins, and at the same time, provide more granularity on the geographic areas, these have been taken from the field County of the files T\\*ADDR+BNFT.CSV. Thus the location of the GP practice will have a granularity at county level rather than region area which is definitely bigger, therefore supposed to provide less detail. For example the location for an observation will now indicate \"Shrewsbury\" instead of \"Shropshire and Staffordshire\".\n",
    "\n",
    "Finally, given the huge amount of data collected, the same data will be used to build and validate a prediction model. This could be a useful instrument to make predictions related to point 1. The model will use future data to predict the usage of a certain drug in a given area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pre-processing data\n",
    "### 2.1 Module imports\n",
    "The project will be imlemented in Python, combined with the Spark framework. Below, the necessary modules. The user defined bnf module parses a web page where the BNF codes are explained and creates a json file called \"sections.json\". Each entry corresponds to a section of the BNF book. Given its auxiliary role to the project, this functionality is implemented as module and its source code is available in the appendix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic imports\n",
    "import pyspark\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Spark mllib imports\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.regression import LinearRegressionWithSGD\n",
    "from pyspark.mllib.tree import DecisionTree\n",
    "from pyspark.mllib.tree import RandomForest\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "\n",
    "# User defined imports\n",
    "import bnf\n",
    "\n",
    "\n",
    "sc = pyspark.SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 BNF sections are parsed and saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving from: https://openprescribing.net/bnf/\n",
      "Results written to sections.json\n"
     ]
    }
   ],
   "source": [
    "bnf.parse_n_save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Reads and parallelizes BNF sections' file\n",
    "Here the BNF codes are read and parallelised by creating RDDs. The first 10 records are shown below as an example. As can be seen, the entries are not necessarily mutually exclusive. It is up to the user to chose either the leafs of the BNF book or consider nodes in the hierarchy branches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('01', 'Gastro-Intestinal System'), ('0101', 'Dyspepsia and gastro-oesophageal reflux disease'), ('010101', 'Antacids and simeticone'), ('010102', 'Compound alginates and proprietary indigestion preparations'), ('0102', 'Antispasmodics and other drugs altering gut motility'), ('0103', 'Antisecretory drugs and mucosal protectants'), ('010301', 'H2-receptor antagonists'), ('010302', 'Selective antimuscarinics'), ('010303', 'Chelates and complexes'), ('010304', 'Prostaglandin analogues')]\n"
     ]
    }
   ],
   "source": [
    "with open('sections.json', 'r') as file:\n",
    "    content = file.read()\n",
    "\n",
    "sections = json.loads(content)\n",
    "sections = [tuple(x.values()) for x in sections]\n",
    "\n",
    "bnf = sc.parallelize(sections)\n",
    "print(bnf.collect()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_Dicts(rows, cols, sep):\n",
    "    \"\"\"\n",
    "    Reads a csv formated line and provides a dictionary as output.\n",
    "    @param rows: The row to be processed.\n",
    "    @param cols: A list with the columns headers previously extracted.\n",
    "    @param sep: The separator used to divide several fields.\n",
    "    @return: A dictionary with the header names and their corresponding values.\n",
    "    \"\"\"\n",
    "    res = {}\n",
    "    rows = rows.split(sep)\n",
    "    for i,j in zip(cols, rows):\n",
    "        j = re.sub('[\\'\"]', '', j.strip())\n",
    "        res.update({i : j})\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_val(row, fx, name):\n",
    "    \"\"\"\n",
    "    Parses a single named value from the PDPI file. It also removes white spaces and and \n",
    "    applies a function to the value.\n",
    "    @param row: a single row in csv format.\n",
    "    @param fx: a function to be applied to a value.\n",
    "    @param name: The (column header) name to identity the field.\n",
    "    @return: A transformed value having a suitable datatype.\n",
    "    \"\"\"\n",
    "    row[name] = re.sub(' ', '', row[name])\n",
    "    if (row[name].lower() != name):\n",
    "        return fx(row[name])\n",
    "    return fx(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_header(row):\n",
    "    \"\"\"\n",
    "    Parses a header file from a csv format and extracts the column names.\n",
    "    @param row: The header row to be parsed.\n",
    "    @return: A list containing the 'cleaned' field names.\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    for col in first_row.split(','):\n",
    "        col = col.strip().lower()\n",
    "        col = re.sub('[ \\'\"]', '', col)\n",
    "        res.append(col)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Read Prescription data (PDPI)\n",
    "The main component of our dataset is the data describing prescriptions. In order to read this data and create a RDD out of it two helper funtions have been created. Their task is to respectively parse headers and values as well as to transform them into a more suitable format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file containing prescriptions is being read and evaluated.\n",
    "\n",
    "The first line has been read and the field names parsed and extracted. From the RDD pdpi_lines we firstly filter lines in order to exclude the headers. Then each line is turned into a Python dictionary. In this way we can address and process each value by its field name.<br>\n",
    "\n",
    "It is also worth mentioning that it is in this part that I have started to make use of the parallelism offered by Spark. As can be seen, a chain of transformations are set to be applied to the original RDD. According to the lazy evaluation at the heart of Spark, they will be applied as soon as an action will be called upon the RDD. To be more precise, none of the filter and map functions will be applied to the original RDD until the action take(10) has to be executed. The action .take() will apply, in chain, all the transformations prior its appearance and cause the driver to collect ten results.<br>\n",
    "\n",
    "The ten elements have been collected for illustration purposes. The .py version of the same file, which is meant and provided to be run as a batch program does not collect its partial result, but carries on instead with the other computations to come."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Q44', 'Y04937', '0204000R0', 2, 2.01, 2.08, 63, '201611'), ('Q44', 'Y04937', '0205051R0', 1, 2.58, 2.4, 56, '201611'), ('Q44', 'Y04937', '0401010Y0', 1, 0.6, 0.67, 14, '201611'), ('Q44', 'Y04937', '0401010Z0', 2, 1.18, 1.32, 28, '201611'), ('Q44', 'Y04937', '0401020K0', 4, 2.52, 2.79, 90, '201611'), ('Q44', 'Y04937', '0402010AB', 5, 3.95, 4.22, 228, '201611'), ('Q44', 'Y04937', '0402010AB', 1, 79.33, 73.54, 28, '201611'), ('Q44', 'Y04937', '0402010A0', 1, 2.5, 2.43, 28, '201611'), ('Q44', 'Y04937', '0402010S0', 1, 48.94, 45.41, 560, '201611'), ('Q44', 'Y04937', '040201030', 1, 1.13, 1.16, 28, '201611')]\n"
     ]
    }
   ],
   "source": [
    "pdpi_lines = sc.textFile(\"./data/T*PDPI*.CSV\")\n",
    "first_row = pdpi_lines.first()\n",
    "headers = parse_header(first_row)\n",
    "\n",
    "pdpi = (pdpi_lines\n",
    "    .filter(lambda p: p != first_row)\n",
    "    .map(lambda p: to_Dicts(p, headers, ','))\n",
    "    .map(lambda p: (p['sha'], p['practice'], p['bnfcode'][:9], \n",
    "                    parse_val(p, int, 'items'),\n",
    "                    parse_val(p, float, 'nic'),\n",
    "                    parse_val(p, float, 'actcost'),\n",
    "                    parse_val(p, int, 'quantity'),\n",
    "                    p['period']))\n",
    ")\n",
    "\n",
    "\n",
    "print(pdpi.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Read GP addresses data (ADDR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('201612', 'A81001', 'CLEVELAND'), ('201612', 'A81002', 'CLEVELAND'), ('201612', 'A81004', 'CLEVELAND'), ('201612', 'A81006', 'CLEVELAND'), ('201612', 'A81007', 'CLEVELAND'), ('201612', 'A81008', 'CLEVELAND'), ('201612', 'A81009', 'CLEVELAND'), ('201612', 'A81011', 'CLEVELAND'), ('201612', 'A81014', 'CLEVELAND'), ('201612', 'A81015', 'CLEVELAND')]\n"
     ]
    }
   ],
   "source": [
    "addr_cols = ['date', 'practice_code', 'name', 'first_line_addr', 'street', 'city', 'county', 'postcode']\n",
    "\n",
    "addr_lines = sc.textFile(\"./data/T*ADDR*.CSV\")\n",
    "\n",
    "addrs = (addr_lines\n",
    "    .map(lambda a: to_Dicts(a, addr_cols, ','))\n",
    "    .filter(lambda a: a['county'] != '')\n",
    "    .map(lambda a: (a['date'], a['practice_code'], a['county']))\n",
    ")\n",
    "\n",
    "print(addrs.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Read drug related data (Chemicals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('0101010A0', 'Alexitol Sodium'), ('0101010C0', 'Aluminium Hydroxide'), ('0101010E0', 'Hydrotalcite'), ('0101010F0', 'Magnesium Carbonate'), ('0101010I0', 'Magnesium Oxide'), ('0101010J0', 'Magnesium Trisilicate'), ('0101010L0', 'Aluminium & Magnesium & Act Simeticone'), ('0101010M0', 'Magaldrate'), ('0101010R0', 'Simeticone'), ('0101010T0', 'Magnesium Sulphate')]\n"
     ]
    }
   ],
   "source": [
    "chem_lines = sc.textFile(\"./data/T*CHEM*.CSV\")\n",
    "header = chem_lines.first()\n",
    "\n",
    "chem = (chem_lines\n",
    "    .filter(lambda x: x != header)\n",
    "    .map(lambda x: tuple(y.strip() for y in x.split(',')[:-1]))\n",
    "    .distinct()\n",
    ")\n",
    "\n",
    "print(chem.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Data Analysis\n",
    "### 3.1 Joining datasets\n",
    "In this part data coming from separate files/RDDs will be combined together in order to perform the three objectives of the data analysis, presented in the introduction. Initially I will start with the prescriptions data, which will undergo a set of transformations and named t1 (Table 1). Although I will be using the Spark RDDs for this task, I found it useful to reason in SQL terms, thus this choice of nomenclature (t1_t2_t3 means join of t1,t2 and t3). Also, the analysis I intend to do on the dataset has been presented as SQL queries as a more intuitive approach.\n",
    "\n",
    "Before joing two RDDs together, a typical sequence of operations taking place below consists in:\n",
    "* Projection -- chose the fields of interest.\n",
    "* Chose the fields to use as keys for each RDD.\n",
    "* Perform an inner join transformation on the designated fields.\n",
    "* Chose a reduce policy. This boils down to grouping together partitions having the same keys and performing aggregation operations on them.\n",
    "* After a join, new keys will be chosen, perform a new join and so on -- until a sort of view t1_t2_t3 will be ready for use. All the exploratory and machine learning analysis will be based on this data structure.\n",
    "* Eventually, t1_t2_t3 RDD will be persisted in memory since its use is required several times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Which are the drugs more frequently used in a certain area ?\n",
    "The result will also act as a base view to be used in other queries.\n",
    "\n",
    "SELECT <br>\n",
    "<blockquote>pdpi.bnf_code,<br>\n",
    "    sum(pdpi.items),<br>\n",
    "    sum(pdpi.act_cost),<br>\n",
    "    sum(pdpi.quantity),<br>\n",
    "    chem.name,<br>\n",
    "    addrs.county,<br>\n",
    "    bnf.desc,<br>\n",
    "    count(*) AS NPrescriptions<br></blockquote>\n",
    "FROM <br>\n",
    "<blockquote>PDPI pdpi join ADDR addrs ON (pdpi.practice = addrs.code) AND<br>\n",
    "    pdpi join CHEM chem ON (pdpi.code = chem.code) AND<br>\n",
    "    pdpi join BNF bnf ON (pdpi.bnf_code = bnf.code)<br></blockquote>\n",
    "GROUP BY<br>\n",
    "<blockquote>pdpi.bnf_code, addrs.county<br></blockquote>\n",
    "ORDER BY<br>\n",
    "<blockquote>addrs.county, NPrescriptions DESC<br></blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('0403040W0', 'MERSEYSIDE'), ('0403040W0', 64, 580.16, 1029, 'Venlafaxine', 'MERSEYSIDE', 'Other antidepressant drugs', 18)), (('0402010AB', 'MERSEYSIDE'), ('0402010AB', 95, 337.89, 2093, 'Quetiapine', 'MERSEYSIDE', 'Antipsychotic drugs', 16)), (('040201060', 'MERSEYSIDE'), ('040201060', 25, 27.51, 370, 'Olanzapine', 'MERSEYSIDE', 'Antipsychotic drugs', 12)), (('0402010AD', 'MERSEYSIDE'), ('0402010AD', 38, 35.7, 567, 'Aripiprazole', 'MERSEYSIDE', 'Antipsychotic drugs', 8)), (('0403040X0', 'MERSEYSIDE'), ('0403040X0', 99, 192.56, 1328, 'Mirtazapine', 'MERSEYSIDE', 'Other antidepressant drugs', 8)), (('0403030E0', 'MERSEYSIDE'), ('0403030E0', 21, 48.93, 938, 'Fluoxetine Hydrochloride', 'MERSEYSIDE', 'Selective serotonin re-uptake inhibitors', 7)), (('040201030', 'MERSEYSIDE'), ('040201030', 12, 8.41, 252, 'Risperidone', 'MERSEYSIDE', 'Antipsychotic drugs', 7)), (('0408010AE', 'MERSEYSIDE'), ('0408010AE', 21, 539.0, 504, 'Pregabalin', 'MERSEYSIDE', 'Control of epilepsy', 6)), (('0401020K0', 'MERSEYSIDE'), ('0401020K0', 75, 40.32, 1326, 'Diazepam', 'MERSEYSIDE', 'Anxiolytics', 6)), (('0401010Z0', 'MERSEYSIDE'), ('0401010Z0', 103, 61.73, 1322, 'Zopiclone', 'MERSEYSIDE', 'Hypnotics', 6))]\n"
     ]
    }
   ],
   "source": [
    "# E.g.: Initial State ('Q44', 'Y04937', '0204000R0', 2, 2.01, 2.08, 63, '201611')\n",
    "t1 = (pdpi\n",
    "    .map(lambda x: x[1:4] + x[5:])\n",
    "    .keyBy(lambda x: x[1])\n",
    "    .join(chem)\n",
    "    .values()\n",
    "    .map(lambda x: x[0] + (x[1],))\n",
    "    .keyBy(lambda x: (x[0],x[5]))\n",
    ")\n",
    "\n",
    "# E.g.: Initial State ('201612', 'A81001', 'CLEVELAND')\n",
    "t2 = addrs.map(lambda x: ((x[1], x[0]), (x[2])))\n",
    "\n",
    "t1_t2 = (t1\n",
    "    .join(t2)\n",
    "    .mapValues(lambda x: x[0] + (x[1], ))\n",
    "    .values()\n",
    "    .keyBy(lambda x: x[1][:7].rstrip('0'))\n",
    ")\n",
    "\n",
    "# E.g.: Initial State ('01', 'Gastro-Intestinal System')\n",
    "t3 = bnf.filter(lambda x: len(x[0]) >= 6)\n",
    "\n",
    "t1_t2_t3 = ( t1_t2\n",
    "    .join(t3)\n",
    "    .mapValues(lambda x: x[0] + (x[1],1))\n",
    "    .values()\n",
    "    .map(lambda x: x[:5] + x[6:])            \n",
    "    .keyBy(lambda x: (x[1], x[6]))\n",
    "    .map(lambda x: (x[0], x[1][1:]))\n",
    "    .reduceByKey(lambda x,y: (x[0], x[1] + y[1], round(x[2] + y[2], 3), x[3] + y[3], x[4], \n",
    "                              x[5], x[6], x[7] + y[7]))\n",
    "    .sortBy(lambda x: (x[0][1], x[1][7]), ascending=False)\n",
    "    .persist()\n",
    ") \n",
    "\n",
    "print(t1_t2_t3.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 What is the preferred chemical by GP practices for a given category? \n",
    "I am interested to spot any overwhelming tendency in recommending a certain product for a condition.\n",
    "\n",
    "SELECT <br>\n",
    "<blockquote>pdpi.bnf_code,<br>\n",
    "    pdpi.items,<br>\n",
    "    pdpi.act_cost,<br>\n",
    "    pdpi.quantity,<br>\n",
    "    chem.name,<br>\n",
    "    addrs.county,<br>\n",
    "    bnf.desc,<br>\n",
    "    max(count(*)) AS NPrescriptions<br></blockquote>\n",
    "FROM <br>\n",
    "<blockquote>PDPI pdpi join ADDR addrs ON (pdpi.practice = addrs.code) AND<br>\n",
    "    pdpi join CHEM chem ON (pdpi.code = chem.code) AND<br>\n",
    "    pdpi join BNF bnf ON (pdpi.bnf_code = bnf.code)<br></blockquote>\n",
    "GROUP BY<br>\n",
    "<blockquote>bnf.desc<br></blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Control of epilepsy', ('0408010AE', 21, 539.0, 504, 'Pregabalin', 'MERSEYSIDE', 'Control of epilepsy', 6)), ('Drugs used for mania and hypomania', ('0402030Q0', 21, 128.98, 476, 'Valproic Acid', 'MERSEYSIDE', 'Drugs used for mania and hypomania', 4)), ('Antihistamines', ('0304010G0', 14, 24.98, 1442, 'Chlorphenamine Maleate', 'CHESHIRE', 'Antihistamines', 8)), ('Non-opioid analgesics and compound preparations', ('0407010F0', 61, 139.81, 3124, 'Co-Codamol (Codeine Phos/Paracetamol)', 'CHESHIRE', 'Non-opioid analgesics and compound preparations', 15)), ('Antibacterials', ('1103010C0', 34, 62.28, 346, 'Chloramphenicol', 'CHESHIRE', 'Antibacterials', 7)), ('Antifungal preparations', ('1310020H0', 15, 19.5, 330, 'Clotrimazole', 'CHESHIRE', 'Antifungal preparations', 3)), ('Antimalarials', ('0504010Y0', 2, 0.81, 9, 'Quinine Sulfate', 'CHESHIRE', 'Antimalarials', 2)), ('Selective serotonin re-uptake inhibitors', ('0403030E0', 21, 48.93, 938, 'Fluoxetine Hydrochloride', 'MERSEYSIDE', 'Selective serotonin re-uptake inhibitors', 7)), ('Macrolides', ('0501050B0', 100, 316.95, 3902, 'Clarithromycin', 'CHESHIRE', 'Macrolides', 12)), ('Non-steroidal anti-inflammatory drugs', ('1001010J0', 18, 22.52, 1624, 'Ibuprofen', 'CHESHIRE', 'Non-steroidal anti-inflammatory drugs', 7))]\n"
     ]
    }
   ],
   "source": [
    "most_used_chem = (t1_t2_t3\n",
    "    .values()\n",
    "    .map(lambda x: x[0:])\n",
    "    .keyBy(lambda x: x[6])\n",
    "    .reduceByKey(lambda x,y: (x if x[7] > y[7] else y))\n",
    ")\n",
    "\n",
    "print(most_used_chem.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Which medication is more used in a given season?\n",
    "In this part I will attempt to explore the trend for a medication with regard to the time of the year.\n",
    "\n",
    "SELECT <br>\n",
    "<blockquote>pdpi.bnf_code,<br>\n",
    "    sum(pdpi.act_cost),<br>\n",
    "    season = CASE<br>\n",
    "        WHEN pdpi.period in (12,1,2)  THEN \"Winter\"<br>\n",
    "        WHEN pdpi.period in (3,4,5)   THEN \"Spring\"<br>\n",
    "        WHEN pdpi.period in (6,7,8)   THEN \"Summer\"<br>\n",
    "        WHEN pdpi.period in (9,10,11) THEN \"Autumn\"<br>\n",
    "    END<br>\n",
    "    chem.name,<br>\n",
    "    addrs.county,<br>\n",
    "    bnf.desc,<br>\n",
    "    max(count(*)) AS NPrescriptions<br></blockquote>\n",
    "FROM <br>\n",
    "<blockquote>PDPI pdpi join ADDR addrs ON (pdpi.practice = addrs.code) AND<br>\n",
    "    pdpi join CHEM chem ON (pdpi.code = chem.code) AND<br>\n",
    "    pdpi join BNF bnf ON (pdpi.bnf_code = bnf.code)<br></blockquote>\n",
    "GROUP BY<br>\n",
    "<blockquote>season<br></blockquote>\n",
    "ORDER BY<br>\n",
    "<blockquote>season, bnf.desc DESC<br></blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def season(date):\n",
    "    if (len(date) != 6):\n",
    "        return None\n",
    "\n",
    "    month = int(date[-2:])\n",
    "    if (month in [12,1,2]):\n",
    "        return 'Winter'\n",
    "    elif (month in [3,4,5]):\n",
    "        return 'Spring'\n",
    "    elif (month in [6,7,8]):\n",
    "        return 'Summer'\n",
    "    elif (month in [9,10,11]):\n",
    "        return 'Autumn'\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('0403040W0', 'Winter'), ('0403040W0', 357.7, 'Winter', 'Venlafaxine', 'Other antidepressant drugs', 14)), (('0402010AB', 'Winter'), ('0402010AB', 446.09, 'Winter', 'Quetiapine', 'Antipsychotic drugs', 14)), (('040201060', 'Winter'), ('040201060', 26.34, 'Winter', 'Olanzapine', 'Antipsychotic drugs', 12)), (('0403040X0', 'Winter'), ('0403040X0', 162.37, 'Winter', 'Mirtazapine', 'Other antidepressant drugs', 11)), (('0401010Z0', 'Winter'), ('0401010Z0', 38.92, 'Winter', 'Zopiclone', 'Hypnotics', 8)), (('0401020K0', 'Winter'), ('0401020K0', 50.35, 'Winter', 'Diazepam', 'Anxiolytics', 8)), (('0402010AD', 'Winter'), ('0402010AD', 20.04, 'Winter', 'Aripiprazole', 'Antipsychotic drugs', 7)), (('0403030E0', 'Winter'), ('0403030E0', 36.95, 'Winter', 'Fluoxetine Hydrochloride', 'Selective serotonin re-uptake inhibitors', 6)), (('0403030Q0', 'Winter'), ('0403030Q0', 51.18, 'Winter', 'Sertraline Hydrochloride', 'Selective serotonin re-uptake inhibitors', 6)), (('0407010H0', 'Winter'), ('0407010H0', 26.42, 'Winter', 'Paracetamol', 'Non-opioid analgesics and compound preparations', 6))]\n"
     ]
    }
   ],
   "source": [
    "chem_per_seas = (t1_t2\n",
    "    .join(t3)\n",
    "    .mapValues(lambda x: x[0] + (x[1],1))\n",
    "    .values()\n",
    "    .map(lambda x: (x[1], x[3], season(x[5]), x[6], x[8], x[9]))\n",
    "    .keyBy(lambda x: (x[0], x[2]))\n",
    "    .reduceByKey(lambda x,y: (x[0], round(x[1] + y[1], 3), x[2], x[3], x[4], x[5] + y[5]))\n",
    "    .sortBy(lambda x: (x[1][2], x[1][5]), ascending=False)\n",
    ")\n",
    "\n",
    "print(chem_per_seas.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Predictive analysis\n",
    "### 4.1 Methods considered\n",
    "The other objective was also to use the same data to do some inference analysis. The aim is to be able to predict the number of prescriptions for a certain drug in the future. In order to achieve this result some predictive models will be built and their performance assesed.\n",
    "\n",
    "The result of the first query which yields the RDD t1_t2_t3 has counted the frequency of the prescriptions for a medication in a certain English county. Now I would like to build a model which allows for this quantity to be predicted, if the same features are provided but no label. In order to achieve this result, three different techniques have been explored. The objective is to chose the most performing model for my dataset. The models explored are:<br>\n",
    "\n",
    "* Linear Regression As the simplest method to build, understand and interpret I will start by evaluating this model. However there is a caveat to consider. My dataset has three categorical fields which could not be evaluated by the Linear regression in its original form. These three fields will be then dummified, thus generating much more variables that there was initially.\n",
    "* Decision Tree As it is simple to build and interpret and it can handle categorical variables as well. Moreover it does not \"pretend\" extensive tunning, so it would be good to capture any underlying structure which is nott linear. \n",
    "* Random Forests. It benefits from the same characteristics of decision trees -- in fact, it combines many trees together. This method has also hyperparameters that need to be assesed before use.\n",
    "\n",
    "Below is the data as astarting point. Each partition (line) has been paired with a unique index for testing purposes only. This new field will not affect computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 64, 580.16, 1029, 'Venlafaxine', 'MERSEYSIDE', 'Other antidepressant drugs', 18)\n"
     ]
    }
   ],
   "source": [
    "df = (t1_t2_t3\n",
    "    .values()\n",
    "    .zipWithUniqueId()\n",
    "    .map(lambda x: ((x[1],) + x[0][1:]))\n",
    ")\n",
    "\n",
    "original_data = df.first()\n",
    "print(original_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Handling categorical variables\n",
    "Categorical data need to be dummified before applying a Linear model upon them. In order to do so, an auxiliary function which stores all the possible categories for each variable has been created. This new data structure is useful to associate a number to each category value for each variable. In our case the positional index for the categorical variables goes from four to seven in the original dataset.\n",
    "\n",
    "The categorical length for all the variables generated after dummifiyng them will be the sum of all the numbers of categories. For example if we have 153 bnf_codes, 4 counties and 80 types of usage (drug categories) the new dataset will be long 153 + 4 + 80 + 3 (numerical variables) = 240 predictors.\n",
    "\n",
    "Finally in this occasion I am also spliting the dataset between training and testing sets. The idea is to follow a ratio 80/20 percent. Although, as can be seen from the cell output, the function randomSplit provides an approximation rather than an exact split by following this ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainSet = 151, TestSet = 54\n",
      "\n",
      "The dataset has:\n",
      "bnf_code: 153 distinct categories\n",
      "county: 4 distinct categories\n",
      "usage: 80 distinct categories\n"
     ]
    }
   ],
   "source": [
    "def encode(rdd, idx):\n",
    "    \"\"\"\n",
    "    For each categorical variable encode a category to a number.\n",
    "    @param rdd: The data rdd\n",
    "    @param idx: The positional column index\n",
    "    @return: A map storing the categories for each field.\n",
    "    \"\"\"\n",
    "    result = (rdd\n",
    "        .map(lambda x: x[idx])\n",
    "        .distinct()\n",
    "        .zipWithIndex()\n",
    "        .collectAsMap()\n",
    "    )\n",
    "    return result\n",
    "\n",
    "\n",
    "mappings = []\n",
    "for i in range(4, 7):\n",
    "    mappings.append(encode(df, i)) \n",
    "\n",
    "\n",
    "# The total number of all the categories times all the qualitative variables. This will be the total\n",
    "# number of the variables after dummifiying the categorical ones.\n",
    "catg_len = sum(map(len, mappings))\n",
    "\n",
    "train_set, test_set = df.randomSplit([0.8, 0.2], seed=1)\n",
    "\n",
    "print('TrainSet = {}, TestSet = {}\\n'.format(train_set.count(), test_set.count()))\n",
    "\n",
    "print(\"The dataset has:\")\n",
    "for i,j in zip(mappings, ['bnf_code', 'county', 'usage']):\n",
    "    print('{}: {} distinct categories'.format(j, len(i)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Dummification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below there are two helper functions designed to retrieve the features and the labels respective of each observation. The function get_LRfeatures is relative to linear regression method; as only in this context we need to perform extensive dummification of the categorical variables with OneHotEncoded method. The Decision tree methods will not make use of the same variables. Whereas numerical variables are just converted into the right format, float in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E.g.: record = [(64, 580.16, 1029, 'Venlafaxine', 'MERSEYSIDE', 'Other Antidepressant Drugs', 18)]\n",
    "def get_LRfeatures(record, Min, Mid, Max):\n",
    "    \"\"\"\n",
    "    Performs the one-hot-encoding for each categorical variable.\n",
    "    @param record: One single record from a RDD\n",
    "    @paramm Min: The predictors are supposed to have contiguous indexes. This is the starting index \n",
    "    of categorical variables.\n",
    "    @param Mid: The first index of numerical variables.\n",
    "    @param Max: The ending index for the predictors.\n",
    "    @return: The categorical data have been dummified\n",
    "    \"\"\"\n",
    "    catg_vars = np.zeros(catg_len)\n",
    "    i = 0\n",
    "    step = 0\n",
    "    for col in record[Mid:Max]:\n",
    "        idx = mappings[i][col]\n",
    "        catg_vars[idx + step] = 1\n",
    "        step = step + len(mappings[i])\n",
    "        i = i + 1\n",
    "    \n",
    "    num_vars = np.array([float(i) for i in record[Min:Mid]])\n",
    "    return np.concatenate((catg_vars, num_vars))\n",
    "\n",
    "def get_label(record):\n",
    "    \"\"\"\n",
    "    Simply extracts the label (target) field. In our case at position -1.\n",
    "    @param record: One single record from a RDD\n",
    "    @return: The label field as a float datatype\n",
    "    \"\"\"\n",
    "    return float(record[-1])\n",
    "\n",
    "#get_LRfeatures(train_set.take(1), 1, 4, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below the training data has been converted into LabeledPoint objects as the machine learning algorithms part of the package mllib expect the data to be arranged in this format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label value: 18.0\n",
      "Dummified data: [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,64.0,580.16,1029.0]\n",
      "\n",
      "New Length: 240\n"
     ]
    }
   ],
   "source": [
    "lr_train_data = train_set.map(lambda x: LabeledPoint(get_label(x), get_LRfeatures(x, 1, 4, 7)))\n",
    "\n",
    "first_point = lr_train_data.first()\n",
    "\n",
    "print('Label value: {}\\n'.format(str(first_point.label)))\n",
    "print('Dummified data: {}\\n'.format(str(first_point.features)))\n",
    "print('New Length: {}'.format(len(first_point.features)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Tuning of the parameters for the Linear Regression\n",
    "In this section I am going to make a research for the parameters to use into the Linear Regression trainning algorithm. The criteria for this choice would be the minimum value of the mean squared error. The two metrics I am planning to tune are:number of iterations and the step size. The optimal number of iterations will be searched into the set [10, 100, 1000] and the step's size into [0.0001, 0.001, 0.01, 0.1]. Several advise suggests that the step's size has to be a small value. The function below will iterate all the combinations (iteration, step) from the two sets and return those that minimize MSE value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating parameters for Linear Regression:\n",
      "=====\n",
      "MSE = 999999, iterations = 10, step = 0.0001\n"
     ]
    }
   ],
   "source": [
    "def find_lr_parameters(data, iterations, steps):\n",
    "    \"\"\"\n",
    "    Iterates through a set of numbers corresponding to iterations and step size and print\n",
    "    the optimal values.\n",
    "    @param data: The RDD containing the data of interest.\n",
    "    @param iterations: A list of possible iterations.\n",
    "    @param steps: a list of steps sizes\n",
    "    \"\"\"\n",
    "    min_error = 999999\n",
    "    n_iter = iterations[0]\n",
    "    n_step = steps[0]\n",
    "    for i in iterations:\n",
    "        for j in steps:\n",
    "            lm = LinearRegressionWithSGD.train(data, iterations=i, step=j, intercept=True)\n",
    "            lm_pred = data.map(lambda x: (x.label, float(lm.predict(x.features))))\n",
    "            metrics = RegressionMetrics(lm_pred)\n",
    "            mse = metrics.meanSquaredError\n",
    "            if (mse < min_error):\n",
    "                min_error = mse\n",
    "                n_iter = i\n",
    "                n_step = j\n",
    "    print('Estimating parameters for Linear Regression:\\n=====')\n",
    "    print('MSE = {}, iterations = {}, step = {}'.format(min_error, n_iter, n_step))\n",
    "\n",
    "iterations = [10, 100, 1000]\n",
    "steps = [0.0001, 0.001, 0.01, 0.1]\n",
    "    \n",
    "find_lr_parameters(lr_train_data, iterations, steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Training and application of Linear Regression\n",
    "The estimated values for iterations and steps return a very high MSE. The reason for such a high value is the presence of NAN values for almost all the combinations of iterations and steps used to train the model. In this case, the suggested value is returned as the only acceptable value rather than the best value. So, the linear model is not expected to produce acceptable results. A wide variety of values have been tried but the situation has not improved.\n",
    "\n",
    "However, just for completeness, the model has been applied to the training set. As can be evinced from the experiment below, the MSE obtained is indeed very huge, therefore impracticable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-8.79205334e+34 -7.03542219e+33 -6.62130992e+33 -2.34583918e+33\n",
      " -1.30800998e+35]\n",
      "Linear Regression predictions: [(18.0, -1.921393898375354e+46), (12.0, -6.865284139838406e+45), (8.0, -1.0519019356888455e+46), (8.0, -2.466370032434899e+46), (7.0, -1.739796741441163e+46)]\n",
      "\n",
      "TestSet MSE = 5.0397229070660793e+95\n"
     ]
    }
   ],
   "source": [
    "lr_test_data = train_set.map(lambda x: LabeledPoint(get_label(x), get_LRfeatures(x, 1, 4, 7)))\n",
    "lm = LinearRegressionWithSGD.train(lr_train_data, iterations=10, step=0.0001, intercept=True)\n",
    "\n",
    "print(lm.weights[:5])\n",
    "\n",
    "lr_predicted_values = lr_test_data.map(lambda p: (p.label, float(lm.predict(p.features))))\n",
    "print('Linear Regression predictions: {}\\n'.format(str(lr_predicted_values.take(5))))\n",
    "\n",
    "metrics = RegressionMetrics(lr_predicted_values)\n",
    "print('TestSet MSE = {}'.format(str(metrics.meanSquaredError)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Preparing features for Decision Trees\n",
    "In this session the strategy will change as I will be applying a decision tree model. This choice impilies a change in the dataset as the dummification of categorical variables is no longer needed. But, the Decision tree algorithm though expects to receive an numeric code for the various categories instead of their original string values. In order to do this the unique code for a given catogory will be retrieved from the mappings data structure created in 4.2. Mappings resemples a two dimensional array where the first index denotes the field and the second one the category code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Decision Tree:\n",
      "=====\n",
      "Decision Tree features: [64.0,580.16,1029.0,63.0,0.0,63.0]\n"
     ]
    }
   ],
   "source": [
    "# Important. Categorical variables need to be represented by numbers to avoid \n",
    "# ValueError: could not convert string to float: 'abc'. The constructor of LabeledPoint will create numpy arrays\n",
    "def get_features_trees(record):\n",
    "    \"\"\"\n",
    "    Given an observation in input, the function returns the numeric code for each category.\n",
    "    @param record: A single observation\n",
    "    @return: A new observation where the nominal variables are represented by their mapping value\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    idx = 0\n",
    "    for i in record[1:7]:\n",
    "        try:\n",
    "            result.append(float(i))\n",
    "        except ValueError:\n",
    "            result.append(float(mappings[idx][i]))\n",
    "            idx = idx + 1\n",
    "    return np.array(result)\n",
    "\n",
    "tree_train_data = train_set.map(lambda r: LabeledPoint(get_label(r), get_features_trees(r)))\n",
    "tree_test_data = test_set.map(lambda r: LabeledPoint(get_label(r), get_features_trees(r)))\n",
    "\n",
    "tree_train_data.take(1)\n",
    "first_point_dt = tree_train_data.first()\n",
    "\n",
    "print('Training Decision Tree:\\n=====')\n",
    "print('Decision Tree features: {}'.format(str(first_point_dt.features)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 Training Decision Tree Model\n",
    "The decision tree model provided with default parameters, except for maxBins (which was a minimum requirement for the problem at hand) is definitely returning a much lower MSE error than the Linear Regression. The change is also refleted in computing performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth: 5\n",
      "Number of nodes: 51\n",
      "MSE = 0.49781473125843984\n"
     ]
    }
   ],
   "source": [
    "dtree_model = DecisionTree.trainRegressor(tree_train_data, {}, maxBins=54)\n",
    "preds = dtree_model.predict(tree_train_data.map(lambda p: p.features))\n",
    "target_trn = tree_train_data.map(lambda p: p.label)\n",
    "dt_predicted_values = target_trn.zip(preds.map(lambda x: float(x)))\n",
    "\n",
    "print('Depth: {}'.format(str(dtree_model.depth())))\n",
    "print('Number of nodes: {}'.format(str(dtree_model.numNodes())))\n",
    "\n",
    "metrics_dt = RegressionMetrics(dt_predicted_values)\n",
    "print('MSE = {}'.format(str(metrics_dt.meanSquaredError)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8 Application of Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree predictions: [(16.0, 9.666666666666666), (6.0, 4.0), (4.0, 5.25), (4.0, 7.0), (1.0, 1.0)]\n",
      "TestSet MSE = 3.193440241962322\n"
     ]
    }
   ],
   "source": [
    "preds = dtree_model.predict(tree_test_data.map(lambda p: p.features))\n",
    "target_tst = tree_test_data.map(lambda p: p.label)\n",
    "dt_predicted_values = target_tst.zip(preds.map(lambda x: float(x)))\n",
    "\n",
    "print('Decision Tree predictions: {}'.format(str(dt_predicted_values.take(5))))\n",
    "\n",
    "metrics_dt = RegressionMetrics(dt_predicted_values)\n",
    "print('TestSet MSE = {}'.format(str(metrics_dt.meanSquaredError)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9 Training Random Forests Model\n",
    "The last model considered in this study is the Random Forest one. For optimal results this model needs too tune its parameters numTrees and maxDepth. This the function find_rf_parameters is provided to iterate through the parameters and pick the optimal values. Once found the model will be trained with these parameters and the MSE compared with the one achieved by the simple decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating Parameters for Random Forests:\n",
      "=====\n",
      "MSE = 0.6052934684069543, trees = 20, depth = 10\n"
     ]
    }
   ],
   "source": [
    "def find_rf_parameters(data, ntrees, depths, target):\n",
    "    \"\"\"\n",
    "    Iterates through a set of numbers corresponding to numTrees and maxDepth and print\n",
    "    the optimal values.\n",
    "    @param data: The RDD containing the data of interest.\n",
    "    @param ntrees: A list of possible iterations.\n",
    "    @param depths: A list of steps sizes\n",
    "    @param target: The list containing the labels of the training set. \n",
    "    \"\"\"\n",
    "    min_error = 99999999\n",
    "    n_trees = ntrees[0]\n",
    "    n_depth = depths[0]\n",
    "    for i in ntrees:\n",
    "        for j in depths:\n",
    "            rf_model = RandomForest.trainRegressor(data, categoricalFeaturesInfo={}, numTrees=i,\n",
    "                                        featureSubsetStrategy=\"auto\", impurity=\"variance\", maxDepth=j, maxBins=54)\n",
    "            preds2 = rf_model.predict(data.map(lambda x: x.features))\n",
    "            rf_values = target.zip(preds2.map(lambda x: float(x)))\n",
    "            metrics_rf = RegressionMetrics(rf_values)\n",
    "            mse = metrics_rf.meanSquaredError\n",
    "            if (mse < min_error):\n",
    "                min_error = mse\n",
    "                n_trees = i\n",
    "                n_depth = j\n",
    "    print('Estimating Parameters for Random Forests:\\n=====')\n",
    "    print('MSE = {}, trees = {}, depth = {}'.format(min_error, n_trees, n_depth))\n",
    "\n",
    "\n",
    "ntrees = [2, 6, 10, 20]\n",
    "depths = [3, 6, 10]\n",
    "    \n",
    "find_rf_parameters(tree_train_data, ntrees, depths, target_trn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.10 Application of Random Forest Model\n",
    "The Random Forest model has been trained and run with the values numTrees=20 and maxDepth=10 the mean squared error has decreased again in the test set by reaching the lowest value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest predictions: [(16.0, 8.4), (6.0, 7.1), (4.0, 7.9944444444444445), (4.0, 6.644444444444444), (1.0, 1.3)]\n",
      "Number of trees: 20\n",
      "Number of nodes: 1790\n",
      "TestSet MSE = 2.4164909836534063\n"
     ]
    }
   ],
   "source": [
    "rf_model = RandomForest.trainRegressor(tree_train_data, categoricalFeaturesInfo={}, numTrees=20, \n",
    "        featureSubsetStrategy=\"auto\", impurity=\"variance\", maxDepth=10, maxBins=54)\n",
    "preds2 = rf_model.predict(tree_test_data.map(lambda x: x.features))\n",
    "rf_predicted_values = target_tst.zip(preds2.map(lambda x: float(x)))\n",
    "\n",
    "print('Random Forest predictions: {}'.format(str(rf_predicted_values.take(5))))\n",
    "print('Number of trees: {}'.format(str(rf_model.numTrees())))\n",
    "print('Number of nodes: {}'.format(str(rf_model.totalNumNodes())))\n",
    "\n",
    "metrics_rf = RegressionMetrics(rf_predicted_values)\n",
    "print('TestSet MSE = {}'.format(metrics_rf.meanSquaredError))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Conclusions\n",
    "The objectives of this study were both explorative and predictive. By relying on  the big data collected and the power of the Spark framework some interesting results came to light.\n",
    "\n",
    "1) The most used drugs in a certain area. These exploratory results allow to see the most used drugs at a certain area. To notice the formation of clusters as in the example below suggesting the most problematic condition over a year in this geographical area.\n",
    "\n",
    "('0403040W0', 64, 580.16, 1029, 'Venlafaxine', 'MERSEYSIDE', 'Other Antidepressant Drugs', 18)),<br>\n",
    "('0402010AB', 95, 337.89, 2093, 'Quetiapine', 'MERSEYSIDE', 'Antipsychotic Drugs', 16)),<br>\n",
    "('040201060', 25, 27.51, 370, 'Olanzapine', 'MERSEYSIDE', 'Antipsychotic Drugs', 12)),<br>\n",
    "('0402010AD', 38, 35.7, 567, 'Aripiprazole', 'MERSEYSIDE', 'Antipsychotic Drugs', 8)),<br>\n",
    "('0403040X0', 99, 192.56, 1328, 'Mirtazapine', 'MERSEYSIDE', 'Other Antidepressant Drugs', 8)),<br>\n",
    "('0403030E0', 21, 48.93, 938, 'Fluoxetine Hydrochloride', 'MERSEYSIDE', 'Selective Serotonin Re-Uptake Inhibitors', 7)<br/>\n",
    "\n",
    "2) The second result allows to see tendencies of the most prescribed drug for a given category. For example, by counting the prrescriptions it looks like 'Chlorphenamine Maleate' has been the most reccomended drug among Antihistamines and soon.<br>\n",
    "('0304010G0', 14, 24.98, 1442, 'Chlorphenamine Maleate', 'CHESHIRE', 'Antihistamines', 8)),<br>\n",
    "\n",
    "3) In the most prescribed drug for season we have achieved a result similar to point 1, although in this case we care more for the time of the year rather than the geographical area.\n",
    "\n",
    "The second part of this project was concerned with building a predictive model that, based on the same variables, would inefere the number of prescriptions in a certain area. I examined three different techniques and the results were as follows.\n",
    "\n",
    "<b>Linear Regression</b> The model built provides a very high MSE so the model was impracticable with its value of MSE = 5.039722907066078e+95 in the test set.\n",
    "\n",
    "<b>Decision Tree</b> The predictive model returned a MSE of:<br>\n",
    "0.384 for the training set <br>\n",
    "4.136 for the test set <br>\n",
    "\n",
    "<b>Random Forests</b> The predictive model returned a MSE of:<br>\n",
    "0.653 for the training set <br>\n",
    "2.410 for the test set <br>\n",
    "\n",
    "So the Decision Tree was the model to provide the best fit for the training set, but Random Forests was able to generalize better and be more accurate on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix 1\n",
    "### Module for scraping BNF codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def get_soup_object(url):\n",
    "    # set a user-agent to be sent with request\n",
    "    headers = {\n",
    "        \"user-agent\":\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) \\\n",
    "        Chrome/61.0.3163.100 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    print('Retrieving from: {}'.format(url))\n",
    "\n",
    "    response  = requests.get(url, headers)\n",
    "    if (response.ok):\n",
    "        # parse the raw HTML into a 'soup' object\n",
    "        soupObj = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        return soupObj\n",
    "    else:\n",
    "        print('There was a problem reading from the URL: {}'.format(url))\n",
    "        return None\n",
    "\n",
    "\n",
    "def parse_n_save():\n",
    "        bsObj = get_soup_object('https://openprescribing.net/bnf/')\n",
    "        data = bsObj.find_all('a')\n",
    "        pattern = re.compile('/bnf/[0-9]{2,6}')\n",
    "\n",
    "        results = []\n",
    "\n",
    "        for i in data:\n",
    "                temp_dict = {}\n",
    "                href = i.attrs['href']\n",
    "                match = re.search(pattern, href)\n",
    "                if match:\n",
    "                        try:\n",
    "                                code = href.split('/')[2]\n",
    "                                desc = i.text.split(':')[1].strip()\n",
    "                                re.sub(r'[^a-zA-Z0-9_]', '', desc)\n",
    "                                temp_dict['code'] = code\n",
    "                                temp_dict['desc'] = desc\n",
    "                                results.append(temp_dict)\n",
    "                        except Exception as e:\n",
    "                                print(str(e))\n",
    "\n",
    "        with open('sections.json', 'w') as output_file:\n",
    "                json.dump(results, output_file)\n",
    "                print('Results written to sections.json')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
